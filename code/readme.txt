RLHF (Reinforcement Learning with Human Feedback) Training Pipeline
This project implements a Reinforcement Learning with Human Feedback (RLHF) pipeline to train a mental health chatbot model. Its main task is to sequentially execute various training stages, from data preparation to reinforcement learning optimization, based on the provided mental health conversation data. The pipeline leverages a pre-trained language model and fine-tunes it through multiple stages to improve its performance in providing emotional support.

Features
Data Preparation (prepare_data): Downloads and formats the dataset, generates SFT (Supervised Fine-Tuning) and preference comparison data files (in JSONL format).

SFT Training (train_sft): Fine-tunes a language model on SFT data (LoRA or full fine-tuning).

Reward Model Training (train_reward): Trains a reward model from preference comparisons.

PPO Reinforcement Learning Training (train_ppo): Trains the model with PPO (Proximal Policy Optimization) using the reward model as a critic.

Model Evaluation (evaluate): Evaluates the model's empathy performance using sentiment analysis.

Usage
One-Click Full Pipeline
To run the entire training pipeline with one command, use the following:

bash
复制
编辑
python main.py
This will execute all the stages in sequence: data preparation, SFT training, reward model training, PPO reinforcement learning training, and model evaluation.

Run Each Stage Individually
You can also choose to run each stage individually with the following commands:

bash
复制
编辑
# Prepare data
python main.py prepare_data --help

# Train SFT
python main.py train_sft --train_file data/sft_train.jsonl ...

# Train reward model
python main.py train_reward --pref_file data/prefs.jsonl ...

# Train PPO model
python main.py train_ppo --sft_ckpt checkpoints/sft --reward_ckpt checkpoints/reward ...

# Evaluate model
python main.py evaluate --ppo_ckpt checkpoints/ppo ...
Parameters
prepare_data: Data preparation stage, downloads and formats the dataset. You can limit the number of SFT pairs generated using the max_pairs parameter.

train_sft: Fine-tunes the language model using SFT data. You can specify the training data file and output path.

train_reward: Trains the reward model based on preference comparison data, using positive and negative samples derived from the SFT data.

train_ppo: Performs further optimization using PPO reinforcement learning on the fine-tuned SFT model. During this process, the reward model serves as the critic.

evaluate: Evaluates the model's ability to provide emotional support, producing an empathy ratio as output.

Environment Dependencies
This project requires the following Python libraries:

torch: PyTorch framework for deep learning model training and inference.

datasets: Hugging Face's datasets library for dataset management.

transformers: Hugging Face's transformers library for loading and fine-tuning pre-trained models.

peft: A library for LoRA-based model fine-tuning.

trl: A library for PPO reinforcement learning training.

nltk: A library for sentiment analysis.

tqdm: A library for displaying progress bars in loops.

To install the dependencies:


pip install torch datasets transformers peft trl nltk tqdm
Training Details
Data Preparation
In the data preparation stage, the script downloads conversation data from the specified dataset and formats it according to different formats (dialogues, question-answer pairs, prompt-response pairs). The formatted data is then saved as SFT data and preference comparison data for further training.

SFT Training
The SFT (Supervised Fine-Tuning) training fine-tunes the pre-trained language model on the formatted conversation data. You can choose between LoRA-based fine-tuning or full fine-tuning. The model learns to generate appropriate responses from the SFT data.

Reward Model Training
The reward model is trained using preference comparison data, where the task is to predict which response (chosen or rejected) is better. The positive samples represent "chosen" responses, and negative samples represent "rejected" responses.

PPO Reinforcement Learning Training
In the PPO reinforcement learning stage, the model is further optimized by interacting with the reward model. At each step, the model generates responses and adjusts based on the feedback provided by the reward model. This process iteratively improves the quality of the model's outputs.

Model Evaluation
In the evaluation stage, the script assesses the model's performance through sentiment analysis, calculating an empathy ratio. This ratio reflects the percentage of positive emotional responses generated by the model during mental health conversations.

Output Directories
All models and data generated during the training and evaluation process will be saved in the following directories:

data/: Stores the prepared data, evaluation results, and other outputs.

checkpoints/: Stores model checkpoints (e.g., SFT model, reward model, PPO model).

License
This code is open-source and can be used, modified, and distributed freely, but please comply with the terms of the open-source license.